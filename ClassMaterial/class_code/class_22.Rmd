---
title: "Class 22 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$



<!--  Please run the code in the  R chunk below once. This will install some
packages and download data and images needed for these exercises.  -->

```{r message=FALSE, warning = FALSE, echo = FALSE, eval = FALSE}


SDS230::download_data("IPED_salaries_2016.rda")

SDS230::download_data("players_born_1901_1950.Rda")

SDS230::download_data("x_y_join.rda")


```




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)
library(dplyr)
library(ggplot2)

#options(scipen=999)


knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```



$\\$




## Overview


 * Poisson regression
 * Data wrangling functions
 


$\\$







## Part 1: Poission regression


In Poisson regression, we try to predict count data; i.e., our predictions are non-negative integers: 0, 1, 2, 3, ...

To explore Poisson regression, let's model how many times the Roy Kent said the word f.ck on each episode of the TV show Ted Lasso. In particular, we will assess whether he said f.ck at a higher rate during episodes when: 
  a. He was coaching soccer.  
  b. When he was dating Keeley Jones.   

This analysis is based on the work of Deepsha Menghani and Julia Silge. 



$\\$




### Part 1.1: Loading the data

The R chunk below loads the data we will need for our analyses. In particular, the data is in a package packaged created by Deepsha Menghani which we will install from GitHub and then load the data into R.

The variables that are of interest for our analysis are: 

1. `F_count_RK`: How many times Roy Kent said F.ck in an episode.   
2. `Coaching_flag`: Whether Roy Kent was coaching in an episode.  
3. `Dating_flag`: Whether Roy Kent was dating Keeley in an episode.  


```{r load_richmond_data}

# devtools::install_github("deepshamenghani/richmondway")

library(richmondway)

data(richmondway)


```



$\\$


### Part 1.2: Visualizing the data

Let's start by visualizing the data. 


```{r viz_richmond}








```


$\\$


### Part 1.3: Building a GLM

Let's now fit a Generalized Linear Model to the data, were we try to predict the number of times Roy Kent say f.ck as a function of whether he was coaching and or dating Keeley. To do this we will use the `glm()` function and set the family to be poisson.  



```{r}






```


Does there appear to be a statistically significant relationship between how many times Roy Kent said f.ck and whether he was coaching and/or dating Keeley? 





$\\$





### Part 1.4: Bootstrap confidence intervals for regression parameters

We can also use the bootstrap to create confidence intervals for the regression parameters. 


```{r boot_richmond}

















```


$\\$




## Part 2: Data cleaning and wrangling with the tidyverse

Let's look at some additional tidyverse packages that are useful for data
cleaning and wrangling. In particular we can look at:

1. `lubridate` for cleaning/processing dates
2. `stringr` for manipulating strings
3. `tidyr` for reshaping data


We might not get through all these functions/packages today but I wanted to make
you aware of them so you can learn more on your own, and we can potentially
revisit some of these packages during the last week of class.


$\\$


### Part 2.1: lubridate for processing dates


Let's try some basic examples by finding the month that major league baseball players played their final game (using players who played from 1901 to 1950).


```{r lubridate}

library(lubridate)

# load the baseball data
load("players_born_1901_1950.Rda")


# get the final game players played which is a string




# convert to a date and extract the month




# plot the months as a barplot



```




$\\$




### Part 2.2: stringr for manipulating strings

The `stringr` package has many useful functions for manipulating strings. Let's just focus on the `str_replace_all(original_string, "old", "new")` function which will replace all instances of the "old" string with the "new" string. 



```{r stringr}

library(stringr)


# replace an occurrence of a substring
#str_replace("String", "old", "new")











# Example: Let's download an article from the internet

base_name <- "https://www.nbcnews.com/politics/2024-election/feels-kind-weird-vance-reflects-looks-ahead-vp-campaign-ends-rcna178606"

article_name <- "politics.html"
download.file(base_name, article_name)

# viewer <- getOption("viewer")
# viewer(article_name)


# read the whole article as a single string
the_article <- readChar(article_name, file.info(article_name)$size)





# replace all occurrences of a string




# write the new article




#viewer("yale_article.html")




```




$\\$




## Pivoting data with tidyr



### Part 3.1: tidyr for pivoting data longer


Let's see if we can compare men and women salary on the same plot using ggplot
by first pivoting our longer.


```{r pivoting_longer}

library(tidyr)

load("IPED_salaries_2016.rda")


# get salaries for men and women
men_women <- IPED_salaries |>
  filter(rank_name == "Full") |>
  select(school, endowment, salary_men, salary_women) |>
  na.omit()


# how can plot men and women salaries on the same plot using ggplot? 

# let's pivot the data longer




# visualize as a boxplot





# visualize as a density plot







```


Does it appear that men and women are being paid differently? 




$\\$






### Part 3.2: tidyr for pivoting data wider


Let's pivot back wider to see if we can come up with more informative plots using ggplot. 



```{r pivoting_wider}


# create the data longer again and mutate on salary difference




# visualize as a boxplot






# visualize as a density






```



Does it appear that men and women are being paid differently? 



$\\$






## Part 4: Joining data frames


Often data of interest is spread across multiple data frames that need to be
joined together into a single data frame for further analyses. We will explore
how to do this using dplyr.


Let's look at a very simple data set to explore joining data frames. 


```{r}

library(dplyr)


load('x_y_join.rda')

x
y



```





$\\$





### Part 4.1: Left join

Left joins keep all rows in the left table.  

Data from right table added when there is the key matches, otherwise NA as added. 

Try to do a left join of the data frames x and y using their keys.


```{r}




```






$\\$






### Part 4.2: Right join


Right joins keep all rows in the right table.  

Data from left table added when there is the key matches, otherwise NA as added. 

Try to do a right join of the data frames x and y using their keys.


```{r}




```





$\\$






### Part 4.3: Inner join


Inner joins only keep rows in which there are matches between the keys in both tables

Try to do an inner join of the data frames x and y using their keys.


```{r}





```





$\\$






### Part 4.4: Full join

Full joins keep all rows in both table.  

NAs are added where there are no matches.




```{r}





```





$\\$






### Part 4.5a: Duplicate keys


Duplicate keys are useful if there is a one-to-many relationship (duplicates are usually in the left table). 

Let's look at two other tables that have duplicate keys



```{r}

x2
y2

nrow(x2)
nrow(y2)


```






$\\$







### Part 4.5b: Duplicate keys


If both tables have duplicate keys you get all possible combinations (Cartesian
product). This is almost always an error! Always check the output dimension
after you join a table because even if there is not a syntax error you might not
get the table you are expecting!


Try doing a left join on the data frames x2 and y2 using only their first keys
(i.e., key1_x and key1_y). Save the joined data frame to an object called
`x2_joined`. Note that `x2_joined` has more rows than the original `x2` data
frame despite the fact that you did a left join!  This is due to duplicate keys
in both x2 and y2.

Usually a mistake was made when a data frame ends up having more rows after a
left join. It is good to check how many rows a data frame has before and after a
join to catch any possible errors.



```{r}

# initial left data frame only has 3 rows



# left join when both the left and right tables have duplicate keys




# output now has more rows than the initial table





```






$\\$






### Part 4.5c: Duplicate keys


To deal with duplicate keys in both tables, we can join the tables **using
multiple keys** in order to make sure that each row is uniquely specified.

Try doing a left join on the data frames x2 and y2 using both the keys. Save the
joined data frame to an object called `x2_joined_mult_keys`. Note that
`x2_joined_mult_keys` has the same number of rows as the original `x2` data
frame which is usually what we want when we do a left join.




```{r}


# initial left data frame only has 3 rows




# join the data frame using multiple keys




# output now only has 3 rows




```





$\\$ 




### Part 4.5: Exploring the flight delays data


Let's look at three data frames from the NYC flights delays data set:

- `flights`: information on flights  
- `airlines`: information on the airlines  
- `weather`: information about the weather  



```{r}


library(nycflights13)

data(flights)
data(airlines)


# get the names from the data frames





# join airlines on to the flights data frame





# delays for each airline








# let's look at the weather too





# join the flights and the weather selecting only arrival delay and time 






# join also including the airport location 







# visualize the regression line to the data predicting delay from wind speed







```





